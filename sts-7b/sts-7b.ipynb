{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chib7krn1FrA"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers\n",
        "!pip install torch\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install hugginface_hub\n",
        "!pip install accelerate --upgrade\n",
        "!pip install bitsandbytes\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes --upgrade\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plVcjMfRPURD"
      },
      "outputs": [],
      "source": [
        "from transformers import LlamaForCausalLM, LlamaTokenizer, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "import datasets\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import accelerate\n",
        "import bitsandbytes\n",
        "\n",
        "from google.colab import userdata\n",
        "my_secret_key = userdata.get('HF_TOKEN')\n",
        "\n",
        "tokenizer_7b = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "\n",
        "tokenizer_7b_chat = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
        "\n",
        "model_7b = LlamaForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-2-7b-hf\",\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "model_7b_chat = LlamaForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "s0HtYjYExF5x"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "import shutil\n",
        "\n",
        "with gzip.open('Sts.gz', 'rb') as f_in:\n",
        "    with open('sts.txt', 'wb') as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "def read_sts_dataset(filename):\n",
        "    with open(filename, 'r') as f:\n",
        "        sts_data = f.readlines()\n",
        "    return sts_data\n",
        "\n",
        "sts_data = read_sts_dataset('sts.txt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KucwJn6_ogXp"
      },
      "outputs": [],
      "source": [
        "def parse_file(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) >= 6:\n",
        "                entry = {\n",
        "                    'score': float(parts[4]),\n",
        "                    'sentence1': parts[5],\n",
        "                    'sentence2': parts[6]\n",
        "                }\n",
        "                data.append(entry)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LntEBVIpwZay"
      },
      "outputs": [],
      "source": [
        "dataset = parse_file(\"sts.txt\")\n",
        "first_150_pairs = dataset[:150]\n",
        "first_50_pairs = dataset[:50]\n",
        "\n",
        "def generate_scores_for_dataset(model, tokenizer, dataset):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "    - A list of dictionaries with each dictionary containing 'sentence1', 'sentence2', 'score' (actual score), and 'predicted_score'.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for pair in dataset:\n",
        "        predicted_score = generate_similarity_score_zero_shot(model, pair['sentence1'], pair['sentence2'], tokenizer)\n",
        "        results.append({\n",
        "            'sentence1': pair['sentence1'],\n",
        "            'sentence2': pair['sentence2'],\n",
        "            'actual_score': pair.get('score'),\n",
        "            'predicted_score': predicted_score\n",
        "        })\n",
        "    return results\n",
        "\n",
        "def generate_similarity_score_zero_shot(model, sentence1, sentence2, tokenizer):\n",
        "    prompt = f\"\"\"Given the two sentences:\\n- \\\"{sentence1}\\\"\\n- \\\"{sentence2},\n",
        "    choose the best score (0 not similar at all, 5 very similar): 0, 1, 2, 3, 4, 5.\n",
        "    The answer is:\"\"\"\n",
        "    model_response = invoke_model_zero_shot(model, prompt, tokenizer)\n",
        "    predicted_score = model_response\n",
        "    return predicted_score\n",
        "\n",
        "def generate_similarity_score_few_shot_2_examples(model, sentence1, sentence2, tokenizer):\n",
        "  prompt = f\"\"\"\n",
        "  Sentence 1: A cat is rubbing against baby's face. \\n\n",
        "  Sentence2: A cat is rubbing against a baby. \\n\n",
        "  Similarity score: 3.800 \\n\n",
        "  Sentence 1: A woman is writing.\t\\n\n",
        "  Sentence 2: A woman is swimming. \\n\n",
        "  Similarity score: 0.500\\n\n",
        "  Given the two sentences:\\n- \\\"{sentence1}\\\"\\n- \\\"{sentence2},\n",
        "  choose the best similarity score (0 not similar at all, 5 very similar):\n",
        "  0, 1, 2, 3, 4, 5. The answer is:\n",
        "  \"\"\"\n",
        "  model_response = invoke_model_few_shot_2_examples(model, prompt, tokenizer)\n",
        "  predicted_score = model_response\n",
        "  return predicted_score\n",
        "\n",
        "def generate_similarity_score_few_shot_4_examples(model, sentence1, sentence2, tokenizer):\n",
        "  prompt = f\"\"\"\n",
        "  Sentence 1: A cat is rubbing against baby's face. \\n\n",
        "  Sentence2: A cat is rubbing against a baby. \\n\n",
        "  Similarity score: 3.800 \\n\n",
        "  Sentence 1: A woman is writing.\t\\n\n",
        "  Sentence 2: A woman is swimming. \\n\n",
        "  Similarity score: 0.500\\n\n",
        "  Sentence 1: A young girl is sitting on Santa's lap.\\n\n",
        "  Sentence 2: A little girl is sitting on Santa's lap.\\n\n",
        "  Similarity score: 4.800\\n\n",
        "  Sentence 1: A grey hound is active in a grassy field.\\n\n",
        "  Sentnece 2: A bunch of bikes racing on a track.\\n\n",
        "  Similairty score: 0.000\n",
        "  Given the two sentences:\\n- \\\"{sentence1}\\\"\\n- \\\"{sentence2},\n",
        "  choose the best similarity score (0 not similar at all, 5 very similar):\n",
        "  0, 1, 2, 3, 4, 5. The answer is:\n",
        "  \"\"\"\n",
        "  model_response = invoke_model_few_shot_4_examples(model, prompt, tokenizer)\n",
        "  predicted_score = model_response\n",
        "  return predicted_score\n",
        "\n",
        "def invoke_model_zero_shot(model, prompt, tokenizer):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    inputs = inputs.to(model.device)\n",
        "    outputs = model.generate(inputs, max_length=100, num_return_sequences=1, temperature=0.5, top_k=50, top_p=0.95, num_beams=1)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True) # [prompt_length:] cuts out the prompt itself while writing in the file\n",
        "    return response\n",
        "\n",
        "def invoke_model_few_shot_2_examples(model, prompt, tokenizer):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    inputs = inputs.to(model.device)\n",
        "    outputs = model.generate(inputs, max_length=250, num_return_sequences=1)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "def invoke_model_few_shot_4_examples(model, prompt, tokenizer):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    inputs = inputs.to(model.device)\n",
        "    outputs = model.generate(inputs, max_length=500, num_return_sequences=1)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "CMAjylhXaGYl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "#free generation\n",
        "\n",
        "df_zero_shot = pd.DataFrame(first_50_pairs)\n",
        "df_zero_shot['Predicted Score LLaMA-7B'] = df_zero_shot.apply(lambda x: generate_similarity_score_zero_shot(model_7b, x['sentence1'], x['sentence2'], tokenizer_7b), axis=1)\n",
        "df_zero_shot['Predicted Score LLaMA-7B-Chat'] = df_zero_shot.apply(lambda x: generate_similarity_score_zero_shot(model_7b_chat, x['sentence1'], x['sentence2'], tokenizer_7b_chat), axis=1)\n",
        "\n",
        "csv_file_path = \"results_zero_shot.csv\"\n",
        "df_zero_shot.to_csv(csv_file_path, index=False)\n",
        "###\n",
        "df_few_shot_2_examples = pd.DataFrame(first_50_pairs)\n",
        "df_few_shot_2_examples['Predicted Score LLaMA-7B'] = df_few_shot_2_examples.apply(lambda x: generate_similarity_score_few_shot_2_examples(model_7b, x['sentence1'], x['sentence2'], tokenizer_7b), axis=1)\n",
        "df_few_shot_2_examples['Predicted Score LLaMA-7B-Chat'] = df_few_shot_2_examples.apply(lambda x: generate_similarity_score_few_shot_2_examples(model_7b_chat, x['sentence1'], x['sentence2'], tokenizer_7b_chat), axis=1)\n",
        "\n",
        "df_few_shot_2_examples.to_csv(\"results_few_shot_2_examples.csv\", index=False)\n",
        "# QUESTION: for x-shot results, should I compare manually or with log probabilities?\n",
        "\n",
        "df_few_shot_4_examples = pd.DataFrame(first_50_pairs)\n",
        "df_few_shot_4_examples['Predicted Score LLaMA-7B'] = df_few_shot_4_examples.apply(lambda x: generate_similarity_score_few_shot_4_examples(model_7b, x['sentence1'], x['sentence2'], tokenizer_7b), axis=1)\n",
        "df_few_shot_4_examples['Predicted Score LLaMA-7B-Chat'] = df_few_shot_4_examples.apply(lambda x: generate_similarity_score_few_shot_4_examples(model_7b_chat, x['sentence1'], x['sentence2'], tokenizer_7b_chat), axis=1)\n",
        "\n",
        "df_few_shot_4_examples.to_csv(\"results_few_shot_4_examples.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from RL hw2 ex3, adapted for the task\n",
        "def get_log_prob_of_similarity_score(model, tokenizer, sentence1, sentence2, actual_score, device):\n",
        "    prompt = f\"\"\"Given the two sentences:\\n- \"{sentence1}\"\\n- \"{sentence2}\"\\n,\n",
        "    choose the best score (0 not similar at all, 5 very similar):\n",
        "    0, 1, 2, 3, 4, 5. The answer is:\"\"\"\n",
        "    score_str = str(actual_score)\n",
        "    input_ids_prompt = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=1024)['input_ids'].to(device)\n",
        "    input_ids_score = tokenizer(score_str, return_tensors='pt')['input_ids'].to(device)\n",
        "\n",
        "    input_ids = torch.cat((input_ids_prompt, input_ids_score[:, 1:]), dim=-1)\n",
        "\n",
        "    attention_mask = (input_ids != tokenizer.eos_token_id).long()\n",
        "    position_ids = attention_mask.cumsum(-1) - 1\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(input_ids=input_ids, attention_mask=attention_mask, position_ids=position_ids)\n",
        "\n",
        "    logits_score = out.logits[:, input_ids_prompt.shape[-1]-1:-1].squeeze()\n",
        "    log_probs = torch.nn.functional.log_softmax(logits_score, dim=-1)\n",
        "    log_probs_score = log_probs.gather(dim=-1, index=input_ids_score[:, 1:].squeeze().unsqueeze(-1)).squeeze()\n",
        "    # mean proabbility across the score token\n",
        "    mean_log_prob_score = log_probs_score.mean().item()\n",
        "\n",
        "    return mean_log_prob_score\n",
        "\n",
        "def evaluate_dataset(data, model, tokenizer, device):\n",
        "    results = []\n",
        "    for pair in data:\n",
        "        log_prob_score = get_log_prob_of_similarity_score(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            sentence1=pair['sentence1'],\n",
        "            sentence2=pair['sentence2'],\n",
        "            actual_score=pair['score'],\n",
        "            device=device\n",
        "        )\n",
        "        result = {\n",
        "            'sentence1': pair['sentence1'],\n",
        "            'sentence2': pair['sentence2'],\n",
        "            'actual_score': pair['score'],\n",
        "            'log_prob_score': log_prob_score\n",
        "        }\n",
        "        results.append(result)\n",
        "    return results\n",
        "\n",
        "evaluated_results_7b = evaluate_dataset(first_150_pairs, model_7b, tokenizer_7b, device)\n",
        "evaluated_results_7b_chat = evaluate_dataset(first_150_pairs, model_7b_chat, tokenizer_7b_chat, device)\n"
      ],
      "metadata": {
        "id": "x9S4Eb2Ng9ad"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_results_to_csv(evaluated_results_7b, evaluated_results_7b_chat, filepath):\n",
        "    df_7b = pd.DataFrame(evaluated_results_7b)\n",
        "    df_7b_chat = pd.DataFrame(evaluated_results_7b_chat)\n",
        "    df_7b = df_7b.rename(columns={'log_prob_score': 'log_prob_score_7b'})\n",
        "    df_7b_chat = df_7b_chat.rename(columns={'log_prob_score': 'log_prob_score_7b_chat'})\n",
        "    df_combined = pd.merge(df_7b, df_7b_chat, on=['sentence1', 'sentence2', 'actual_score'])\n",
        "\n",
        "    df_combined.to_csv(filepath, index=False)\n",
        "\n",
        "combine_results_to_csv(evaluated_results_7b, evaluated_results_7b_chat, 'combined_log_probs.csv')\n"
      ],
      "metadata": {
        "id": "pi0NXHkwnprD"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_combined = pd.read_csv('combined_log_probs.csv')\n",
        "\n",
        "average_log_prob_7b = df_combined['log_prob_score_7b'].mean()\n",
        "average_log_prob_7b_chat = df_combined['log_prob_score_7b_chat'].mean()\n",
        "\n",
        "variance_log_prob_7b = df_combined['log_prob_score_7b'].var()\n",
        "variance_log_prob_7b_chat = df_combined['log_prob_score_7b_chat'].var()\n",
        "\n",
        "print(f\"Average Log Probability Scores:\")\n",
        "print(f\"LLaMA 7B: {average_log_prob_7b}, LLaMA 7B-Chat: {average_log_prob_7b_chat}\")\n",
        "print(f\"\\nVariance of Log Probability Scores:\")\n",
        "print(f\"LLaMA 7B: {variance_log_prob_7b}, LLaMA 7B-Chat: {variance_log_prob_7b_chat}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W43KXrXauZEK",
        "outputId": "604871c5-d45f-4740-c021-987b3301b9e6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Log Probability Scores:\n",
            "LLaMA 7B: -2.7595125201501345, LLaMA 7B-Chat: -4.387921319196098\n",
            "\n",
            "Variance of Log Probability Scores:\n",
            "LLaMA 7B: 0.052189281254898856, LLaMA 7B-Chat: 0.20917896683272616\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}